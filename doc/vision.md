# Техническое видение проекта

## Описание
Техническая документация для разработки ИИ-ассистента в виде Telegram-бота для первичных консультаций клиентов.

---

## Технологии

### Язык и Runtime
- **Python 3.11+** - основной язык разработки
- Строгая типизация через type hints где возможно

### Внешние сервисы
- **Telegram Bot API** - платформа взаимодействия
- **OpenRouter** - провайдер для доступа к LLM (через OpenAI-совместимый client)

### Библиотеки
- `aiogram` - асинхронная работа с Telegram Bot API
- `openai` - клиент для OpenRouter API
- `python-dotenv` - управление переменными окружения

### Хранение данных
- **Без БД** - все данные компании в системном промпте
- История диалогов - в памяти (dict/list структуры)

### Инструменты разработки
- **uv** - управление зависимостями и виртуальным окружением
- **pytest** - фреймворк для тестирования
- **make** - автоматизация команд (build, test, run)
- **Docker** - контейнеризация для деплоя

### Требования к окружению
- Python 3.11+
- Docker 20.10+
- Make

---

## Принципы разработки

### Философия
- **KISS** - максимально простые решения
- **DRY** - избегаем дублирования кода
- **Читаемость > хитрость** - код пишем для людей
- **Минимум абстракций** - только необходимое

### Стандарты кода
- **PEP 8** - стиль кодирования Python (следуем вручную)
- **Type hints** - где логично (параметры функций, возвраты)
- **Английский** - для кода и комментариев
- **Русский** - для пользовательских сообщений бота

### Тестирование
- `pytest` для автоматических тестов
- Фокус на критичные сценарии (обработка сообщений, LLM интеграция)
- Минимальное покрытие для MVP - тестируем то, что может сломаться

### Управление зависимостями
- Минимум внешних библиотек
- Только проверенные и популярные пакеты
- Фиксация версий в pyproject.toml

---

## Структура проекта

```
cursor-homework/
├── src/                 # Исходный код приложения
│   ├── __init__.py
│   ├── bot.py           # Точка входа, инициализация и запуск бота
│   ├── handlers.py      # Обработчики команд (/start) и сообщений
│   └── llm.py           # Интеграция с OpenRouter через OpenAI client
├── tests/               # Автоматические тесты
│   ├── __init__.py
│   ├── test_handlers.py # Тесты обработчиков
│   └── test_llm.py      # Тесты LLM интеграции
├── .env                 # Переменные окружения (НЕ в git)
├── .env.example         # Шаблон переменных окружения
├── .gitignore
├── pyproject.toml       # Конфигурация uv и зависимости проекта
├── Makefile             # Команды автоматизации
├── Dockerfile           # Образ для деплоя
├── README.md
└── doc/                 # Документация
    ├── product-idea.md
    └── vision.md
```

### Принципы организации
- **Плоская структура** - избегаем глубокой вложенности
- **Модульность** - каждый файл отвечает за свою область
- **Простота** - понятно новичку с первого взгляда

---

## Архитектура проекта

### Компоненты системы

1. **bot.py** - точка входа
   - Инициализация Telegram бота
   - Регистрация handlers
   - Запуск polling

2. **handlers.py** - обработчики событий
   - Команды (/start, /help)
   - Обработка текстовых сообщений
   - Управление историей диалога

3. **llm.py** - интеграция с LLM
   - Взаимодействие с OpenRouter через OpenAI client
   - Формирование запросов (system prompt + history)
   - Получение ответов от модели

4. **Хранилище диалогов** - in-memory структура
   - Словарь: `{chat_id: list[dict]}`
   - Хранится в памяти процесса

### Поток обработки сообщения

```
Пользователь → Telegram API → handlers.py → llm.py → OpenRouter API
                                    ↑                       ↓
Пользователь ← Telegram API ← handlers.py ← llm.py ← OpenRouter API
```

### Хранение истории диалога

- Структура: `conversation_history = {chat_id: [messages]}`
- Формат сообщения: `{"role": "user"|"assistant", "content": "текст"}`
- При перезапуске бота история теряется (приемлемо для MVP)

### Обработка ошибок

- `try-except` блоки вокруг критичных операций
- **Логирование**: запись ошибки в лог с деталями
- **Пользователю**: понятное сообщение без технических деталей
  - Пример: "Извините, произошла ошибка. Попробуйте чуть позже."
- Без retry-логики для MVP

---

## Модель данных

### История диалога (in-memory)

```python
# Сообщения диалога
conversation_history = {
    123456: [  # chat_id
        {"role": "user", "content": "Какие услуги вы предлагаете?"},
        {"role": "assistant", "content": "Мы предлагаем..."}
    ]
}

# Метаданные диалога
conversation_meta = {
    123456: {  # chat_id
        "username": "john_doe",
        "started_at": "2026-01-07T10:30:00"
    }
}
```

### Формат сообщений для LLM

Соответствует OpenAI Chat Completions API:
```python
{
    "role": "user" | "assistant" | "system",
    "content": "текст сообщения"
}
```

### Системный промпт

- **Хранение**: переменная окружения `SYSTEM_PROMPT` в `.env`
- **Формат**: простой текст (многострочная строка в .env)
- **Содержание**: описание компании, услуг, инструкции для LLM
- **Загрузка**: один раз при старте бота из `os.getenv("SYSTEM_PROMPT")`

### Команды бота

- `/start` - приветственное сообщение и начало диалога

---

## Работа с LLM

### Провайдер и клиент
- **OpenRouter** - универсальный прокси для доступа к различным LLM
- **Клиент**: OpenAI Python SDK с custom `base_url`
- **Аутентификация**: API ключ OpenRouter

### Конфигурация (через .env)
```bash
# OpenRouter
OPENROUTER_API_KEY=sk-or-...
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# Модель и параметры
LLM_MODEL=openai/gpt-3.5-turbo
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=500

# Системный промпт
SYSTEM_PROMPT="Вы - ИИ-ассистент компании... (многострочный текст)"

# История диалога
CONVERSATION_HISTORY_LIMIT=10  # последние N сообщений

# Retry логика
LLM_RETRY_ATTEMPTS=3
```

### Формирование запроса
```python
messages = [
    {"role": "system", "content": system_prompt},
    *conversation_history[chat_id][-CONVERSATION_HISTORY_LIMIT:]
]
```

### Обработка ответа
- Получаем полный ответ (без streaming)
- Сохраняем в историю диалога
- Отправляем пользователю через Telegram

### Retry при ошибках
- Количество попыток задается в `LLM_RETRY_ATTEMPTS`
- Пауза между попытками: 1 секунда
- После исчерпания попыток - сообщение пользователю об ошибке

---

## Мониторинг

### Подход
Для MVP используем только логирование. Никаких дополнительных инструментов мониторинга.

### Логирование
- Все события записываются в stdout
- Docker автоматически собирает логи контейнера
- Просмотр: `docker logs <container_name>`

### Уровни логирования
- **INFO** - обычные события (старт бота, получение сообщения)
- **WARNING** - потенциальные проблемы (retry попытки)
- **ERROR** - критические ошибки (падение LLM, ошибки API)

### Что логируем
- Старт/остановка бота
- Получение сообщений (chat_id, username)
- Запросы к LLM (без содержимого для экономии места)
- Ошибки и исключения (с полным traceback)
- Retry попытки при сбоях

---

## Сценарии работы

### 1. Основной сценарий - диалог с клиентом
1. Пользователь отправляет текстовое сообщение
2. Бот сохраняет сообщение в историю диалога
3. Бот формирует запрос к LLM (system prompt + история)
4. Бот получает ответ от LLM
5. Бот сохраняет свой ответ в историю
6. Бот отправляет ответ пользователю

История диалога накапливается автоматически в памяти по `chat_id`.

### 2. Нетекстовые сообщения
Если пользователь отправляет фото, файл, стикер, голосовое или другой нетекстовый контент:
- Бот отвечает: "Контент не распознан. Пожалуйста, напишите текстовое сообщение."

### 3. Обработка ошибок LLM
1. При ошибке запроса к LLM API → автоматический retry
2. Количество попыток определяется `LLM_RETRY_ATTEMPTS` из .env
3. Пауза между попытками: 1 секунда
4. Если все попытки исчерпаны:
   - Пользователю: "Извините, произошла ошибка. Попробуйте позже."
   - В лог: полная информация об ошибке с traceback

---

## Деплой

### Контейнеризация

**Dockerfile**
- Базовый образ: `python:3.11-slim`
- Установка зависимостей через `uv`
- Копирование исходного кода
- ENTRYPOINT: запуск бота

**docker-compose.yml**
- Упрощает запуск и управление контейнером
- Монтирование .env файла
- Автоматический restart при падении

### Переменные окружения

Конфигурация передается через `.env` файл:
- Создать `.env` на основе `.env.example`
- Заполнить все необходимые ключи (Telegram, OpenRouter)
- Файл монтируется в контейнер через docker-compose

### Команды (через Makefile)

```bash
make install      # Установка зависимостей (для разработки)
make test         # Запуск тестов
make build        # Сборка Docker образа
make up           # Запуск через docker-compose
make down         # Остановка контейнера
make logs         # Просмотр логов
make restart      # Перезапуск
```

### Процесс деплоя

**Локальная разработка:**
1. Клонировать репозиторий
2. Создать `.env` из `.env.example`
3. `make install` - установка зависимостей
4. `make test` - проверка тестов
5. `make up` - запуск бота

**Деплой в облако (будущее):**
1. Подключиться к серверу
2. Клонировать репозиторий
3. Создать `.env` с продакшн настройками
4. `make build && make up`
5. Проверить логи: `make logs`

---

## Конфигурирование

### Принцип
Все настройки через переменные окружения в `.env` файле. Никакой валидации - если параметр отсутствует, приложение упадет с ошибкой.

### Структура .env

```bash
# Telegram Bot
TELEGRAM_BOT_TOKEN=123456:ABC-DEF1234ghIkl-zyx57W2v1u123ew11

# OpenRouter API
OPENROUTER_API_KEY=sk-or-v1-abc123def456...
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# LLM настройки
LLM_MODEL=openai/gpt-3.5-turbo
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=500

# Системный промпт
SYSTEM_PROMPT="Вы - ИИ-ассистент компании... (многострочный текст)"

# История диалога
CONVERSATION_HISTORY_LIMIT=10

# Обработка ошибок
LLM_RETRY_ATTEMPTS=3

# Логирование
LOG_LEVEL=INFO
```

### Загрузка конфигурации
- Библиотека `python-dotenv` загружает `.env` при старте
- Доступ через `os.getenv("PARAMETER_NAME")`
- Обязательные параметры: если отсутствуют, Python выбросит исключение

### .env.example
Шаблон `.env.example` с комментариями для документации всех доступных параметров.

---

## Логгирование

### Библиотека
Встроенный модуль `logging` из Python стандартной библиотеки.

### Формат логов
Текстовый формат для читаемости:
```
2026-01-07 15:30:45 [INFO] Bot started successfully
2026-01-07 15:31:12 [INFO] Message from user @john_doe (123456): "Какие услуги вы предлагаете?"
2026-01-07 15:31:13 [INFO] Sending request to LLM (model: gpt-3.5-turbo)
2026-01-07 15:31:15 [INFO] LLM response received (tokens: 150)
2026-01-07 15:32:10 [ERROR] LLM API error: Connection timeout
```

### Уровни логирования
Управляется через `LOG_LEVEL` в .env:
- **DEBUG** - детальная отладка (все события)
- **INFO** - стандартные события (по умолчанию)
- **WARNING** - предупреждения (retry попытки)
- **ERROR** - ошибки с полным traceback

### Что логируем

**Обычные события (INFO):**
- Старт и остановка бота
- Получение сообщений: chat_id, username, **текст сообщения**
- Запросы к LLM: модель, параметры
- Ответы от LLM: статус, количество токенов

**Предупреждения (WARNING):**
- Retry попытки при ошибках LLM
- Нетекстовые сообщения от пользователей

**Ошибки (ERROR):**
- Ошибки LLM API с полным traceback
- Критические ошибки Telegram API
- Необработанные исключения

### Вывод и ротация
- **Вывод**: stdout (стандартный вывод)
- **Сбор**: Docker автоматически собирает логи контейнера
- **Просмотр**: `docker logs <container_name>` или `make logs`
- **Ротация**: управляется Docker (не настраиваем в приложении)

---

## Заключение

Данное техническое видение следует принципу **KISS** - максимальная простота без потери функциональности. Документ будет дополняться по мере развития проекта.

**Версия**: 1.0  
**Дата**: 2026-01-07  
**Автор**: Чесноков Антон Николаевич

